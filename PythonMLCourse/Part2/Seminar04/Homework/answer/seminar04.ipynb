{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0lggLVskQD5"
      },
      "source": [
        "## Реализация собственного нейросетевого пакета для запуска и обучения нейронных сетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1YWQGEgkQD5"
      },
      "source": [
        "Задание состоит из трёх частей:\n",
        "1. Реализация прямого вывода нейронной сети (5 баллов)\n",
        "2. Реализация градиентов по входу и распространения градиента по сети (5 баллов)\n",
        "3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети (10 баллов)\n",
        "\n",
        "Дополнительные баллы можно получить при реализации обучения сети со свёрточными слоями (10 баллов), с транспонированной свёрткой (10 баллов), дополнительного оптимизатора (5 баллов)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRW4LGJAkQD6"
      },
      "source": [
        "###  1. Реализация вывода собственной нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqrk5E5rkQD6"
      },
      "source": [
        "1.1 Внимательно ознакомьтесь с интерфейсом слоя. Любой слой должен содержать как минимум три метода:\n",
        "- конструктор\n",
        "- прямой вывод\n",
        "- обратный вывод, производные по входу и по параметрам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dIesa7tUkQD6"
      },
      "outputs": [],
      "source": [
        "class Layer(object):\n",
        "    def __init__(self):\n",
        "        self.name = 'Layer'\n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def backward(self, input_data):\n",
        "        return [self.grad_x(input_data), self.grad_param(input_data)]\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        pass\n",
        "    def grad_param(self, input_data):\n",
        "        return []\n",
        "\n",
        "    def update_param(self, grads, learning_rate):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCaUeap5kQD7"
      },
      "source": [
        "1.2 Ниже предствален интерфейс класса  Network. Обратите внимание на реализацию метода predict, который последовательно обрабатывает входные данные слой за слоем."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAwC60e6kQD8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "import copy\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, layers, loss=None):\n",
        "        self.name = 'Network'\n",
        "        self.layers = layers\n",
        "        self.loss = loss\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return self.predict(input_data)\n",
        "\n",
        "    def grad_x(self, input_data, labels):\n",
        "        current_input = input_data\n",
        "\n",
        "        current_input = self.loss.grad_x(current_input,labels)\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "\n",
        "            current_input_prev = copy.deepcopy(current_input)\n",
        "            current_input = layer.grad_x(current_input)\n",
        "            print('prev :', current_input_prev)\n",
        "            print(layer.name, current_input)\n",
        "            current_input = np.dot(current_input_prev,current_input)\n",
        "\n",
        "        return current_input\n",
        "\n",
        "    def grad_param(self, input_data, labels):\n",
        "        lst = []\n",
        "        for layer in self.layers:\n",
        "          if callable(layer.grad_param):\n",
        "            lst.append(layer.grad_param(input_data))\n",
        "          else:\n",
        "            lst.append(0)\n",
        "        return lst\n",
        "\n",
        "    def update(self, grad_list, learning_rate):\n",
        "        for i in range(len(self.layers)):\n",
        "          if callable(layer.update_param):\n",
        "            layer.update_param(grad_list[i], learning_rate)\n",
        "\n",
        "    def predict(self, input_data):\n",
        "        current_input = input_data\n",
        "        for layer in self.layers:\n",
        "            current_input = layer.forward(current_input)\n",
        "        return current_input\n",
        "\n",
        "    def calculate_loss(self, input_data, labels):\n",
        "        return self.loss.forward(self.predict(input_data), labels)\n",
        "\n",
        "    def train_step(self, input_data, labels, learning_rate=0.001):\n",
        "        grad_list = self.grad_param(input_data, labels)\n",
        "        self.update(grad_list, learning_rate)\n",
        "\n",
        "\n",
        "    def fit(self, trainX, trainY, validation_split=0.25,\n",
        "            batch_size=1, nb_epoch=1, learning_rate=0.01):\n",
        "\n",
        "        train_x, val_x, train_y, val_y = train_test_split(trainX, trainY,\n",
        "                                                          test_size=validation_split,\n",
        "                                                          random_state=42)\n",
        "        for epoch in range(nb_epoch):\n",
        "            #train one epoch\n",
        "            for i in tqdm(range(int(len(train_x)/batch_size))):\n",
        "                batch_x = train_x[i*batch_size: (i+1)*batch_size]\n",
        "                batch_y = train_y[i*batch_size: (i+1)*batch_size]\n",
        "                self.train_step(batch_x, batch_y, learning_rate)\n",
        "            #validate\n",
        "            val_accuracy = self.evaluate(val_x, val_y)\n",
        "            print('%d epoch: val %.2f' %(epoch+1, val_accuracy))\n",
        "\n",
        "    def evaluate(self, testX, testY):\n",
        "        y_pred = np.argmax(self.predict(testX), axis=1)\n",
        "        y_true = np.argmax(testY, axis=1)\n",
        "        val_accuracy = np.sum((y_pred == y_true))/(len(y_true))\n",
        "        return val_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XosUbV0qkQD8"
      },
      "source": [
        "#### 1.1 Необходимо реализовать метод forward для вычисления следующих слоёв:\n",
        "\n",
        "- DenseLayer\n",
        "- ReLU\n",
        "- Softmax\n",
        "- FlattenLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "txUZDBqlkQD9"
      },
      "outputs": [],
      "source": [
        "#импорты\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9jF04TBkQD9"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(Layer):\n",
        "    def __init__(self, input_dim, output_dim, W_init=None, b_init=None):\n",
        "        self.name = 'Dense'\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        if W_init is None or b_init is None:\n",
        "            self.W = np.random.random((input_dim, output_dim))\n",
        "            self.b = np.zeros(output_dim, 'float32')\n",
        "        else:\n",
        "            self.W = W_init\n",
        "            self.b = b_init\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return np.dot(input_data, self.W) + self.b\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        return self.W.T\n",
        "\n",
        "    def grad_b(self, input_data):\n",
        "        if (input_data.ndim == 1):\n",
        "          return np.identity(self.output_dim)\n",
        "        else:\n",
        "          res = []\n",
        "          for i in range(input_data.shape[0]):\n",
        "            res.append(np.identity(self.output_dim))\n",
        "          return np.array(res)\n",
        "\n",
        "    def grad_W(self, input_data):\n",
        "        E = np.identity(self.output_dim)\n",
        "        if (input_data.ndim == 1):\n",
        "          res = E*input_data[0]\n",
        "          for i in range(1,len(input_data)):\n",
        "            res = np.concatenate((res, E*input_data[i]), axis=1)\n",
        "          return res\n",
        "        else:\n",
        "          res = []\n",
        "          for i in range(input_data.shape[0]):\n",
        "            row = input_data[i].flatten()\n",
        "            tmp = E*row[0]\n",
        "            for j in range(1,len(row)):\n",
        "              tmp = np.concatenate((tmp, E*row[j]), axis=1)\n",
        "            res.append(tmp)\n",
        "          return np.array(res)\n",
        "\n",
        "    def update_W(self, grad, learning_rate):\n",
        "        self.W -= learning_rate * np.mean(grad, axis=0).reshape(self.W.shape)\n",
        "\n",
        "    def update_b(self, grad,  learning_rate):\n",
        "        self.b -= learning_rate * np.mean(grad, axis=0)\n",
        "\n",
        "    def update_param(self, params_grad, learning_rate):\n",
        "        self.update_W(params_grad[0], learning_rate)\n",
        "        self.update_b(params_grad[1], learning_rate)\n",
        "\n",
        "    def grad_param(self, input_data):\n",
        "        return [self.grad_W(input_data), self.grad_b(input_data)]\n",
        "\n",
        "class ReLU(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'ReLU'\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return np.maximum(0,input_data)\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        dx = (input_data > 0) * 1\n",
        "        if (dx.ndim == 1):\n",
        "          return np.diag(dx)\n",
        "        else:\n",
        "          res = []\n",
        "          for i in range(dx.shape[0]):\n",
        "            res.append(np.diag(dx[i].flatten()))\n",
        "          return np.array(res)\n",
        "\n",
        "\n",
        "class Softmax(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Softmax'\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return np.exp(input_data)/np.sum(np.exp(input_data), axis=-1, keepdims=True)\n",
        "\n",
        "    def grad_x(self, input_data):\n",
        "        dx = self.forward(input_data)\n",
        "\n",
        "        if (dx.ndim == 1):\n",
        "          return np.diag(dx) - dx.reshape(-1,1) @ dx.reshape(1,-1)\n",
        "\n",
        "        else:\n",
        "          res = []\n",
        "          for i in range(dx.shape[0]):\n",
        "            res.append(np.diag(dx[i]) - dx[i].reshape(-1,1) @ dx[i].reshape(1,-1))\n",
        "          return np.array(res)\n",
        "\n",
        "\n",
        "\n",
        "class FlattenLayer(Layer):\n",
        "    def __init__(self):\n",
        "        self.name = 'Flatten'\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        return input_data.reshape((input_data.shape[0], -1))\n",
        "\n",
        "    def grad_x(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUUCrW9rkQD-"
      },
      "source": [
        "#### 1.2 Реализуйте теперь свёрточный слой и транспонированную свёртку  (опционально)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yh2SGwJlkQD-"
      },
      "outputs": [],
      "source": [
        "class Conv2DLayer(Layer):\n",
        "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3,\n",
        "                 padding='same', stride=1, K_init=None, b_init=None):\n",
        "        # padding: 'same' или 'valid'\n",
        "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
        "        # Работаем с единообразным сдвигом, поэтому stride - одно число\n",
        "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
        "        self.name = 'Conv2D'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.kernel = K_init\n",
        "        self.bias = b_init\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "    def forward(self, input_data):\n",
        "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
        "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
        "        # Нужно заполнить Numpy-тензор out\n",
        "        out = np.empty([])\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def grad_x(self):\n",
        "        pass\n",
        "    def grad_kernel(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVKhcYb_kQD-"
      },
      "outputs": [],
      "source": [
        "class Conv2DTrLayer(Layer):\n",
        "    def __init__(self, kernel_size=3, input_channels=2, output_channels=3,\n",
        "                 padding=0, stride=1, K_init=None, b_init=None):\n",
        "        # padding: число (сколько отрезать от модифицированной входной карты)\n",
        "        # Работаем с квадратными ядрами, поэтому kernel_size - одно число\n",
        "        # stride - одно число (коэффициент расширения)\n",
        "        # Фильтр размерности [kernel_size, kernel_size, input_channels, output_channels]\n",
        "        self.name = 'Conv2DTr'\n",
        "        self.kernel_size = kernel_size\n",
        "        self.input_channels = input_channels\n",
        "        self.output_channels = output_channels\n",
        "        self.kernel = K_init\n",
        "        self.bias = b_init\n",
        "        self.padding = padding\n",
        "        self.stride = stride\n",
        "    def forward(self, input_data):\n",
        "        # На входе - четырехмерный тензор вида [batch, input_channels, height, width]\n",
        "        # Вначале нужно проверить на согласование размерностей входных данных и ядра!\n",
        "        # Нужно заполнить Numpy-тензор out\n",
        "        out = np.empty([])\n",
        "        return out\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        pass\n",
        "    def grad_x(self):\n",
        "        pass\n",
        "    def grad_kernel(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjMpVmM_kQD-"
      },
      "source": [
        "#### 1.4 Теперь настало время теста.\n",
        "#### Если вы всё сделали правильно, то запустив следующие ячейки у вас должна появиться надпись: Test PASSED\n",
        "\n",
        "Переходить к дальнейшим заданиям не имеем никакого смысла, пока вы не добьётесь прохождение теста\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QDQUgDAskQD_"
      },
      "source": [
        "#### Чтение данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IVJ3wX27kQD_",
        "outputId": "a8733289-ca4a-47cd-df93-98be4fa4a0cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 1, 28, 28) (60000, 10) (10000, 1, 28, 28) (10000, 10)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)  # for reproducibility\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)\n",
        "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kx5pbvpbkQD_"
      },
      "source": [
        "#### Подготовка моделей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEimO2A4FpGW"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==1.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqoZ4tcxFtnA"
      },
      "outputs": [],
      "source": [
        "!pip install keras==2.2.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxd-eqgTkQD_",
        "outputId": "51e1b915-8875-44b3-bb83-a7b51f94dd82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.4\n"
          ]
        }
      ],
      "source": [
        "import keras\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Input, ReLU\n",
        "from keras.layers import Convolution2D, Conv2D, MaxPooling2D\n",
        "\n",
        "print(keras.__version__)\n",
        "\n",
        "def get_keras_model():\n",
        "    input_image = Input(shape=(1, 28, 28))\n",
        "    flatten = Flatten()(input_image)\n",
        "    dense1 = Dense(10, activation='softmax')(flatten)\n",
        "    model = Model(inputs=input_image, outputs=dense1)\n",
        "\n",
        "    from keras.optimizers import Adam, SGD\n",
        "    sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=sgd,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(X_train, Y_train, validation_split=0.25,\n",
        "                        batch_size=32, nb_epoch=2, verbose=1)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4JmvpT2CkQEA"
      },
      "outputs": [],
      "source": [
        "def get_our_model(keras_model):\n",
        "    flatten = FlattenLayer()\n",
        "    dense = DenseLayer(196, 10, W_init=keras_model.get_weights()[0],\n",
        "                       b_init=keras_model.get_weights()[1])\n",
        "    softmax = Softmax()\n",
        "    net = Network([flatten, dense, softmax])\n",
        "    return net"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEbXKz6ZkQEA",
        "outputId": "6e30236d-5b8a-4f0e-e3a2-f6da3185f841"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 45000 samples, validate on 15000 samples\n",
            "Epoch 1/2\n",
            "45000/45000 [==============================] - 3s 56us/step - loss: 0.4498 - acc: 0.8769 - val_loss: 0.3334 - val_acc: 0.9082\n",
            "Epoch 2/2\n",
            "45000/45000 [==============================] - 2s 49us/step - loss: 0.3255 - acc: 0.9081 - val_loss: 0.3103 - val_acc: 0.9146\n"
          ]
        }
      ],
      "source": [
        "keras_model = get_keras_model()\n",
        "our_model = get_our_model(keras_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjdsyY0QkQEA"
      },
      "outputs": [],
      "source": [
        "keras_prediction = keras_model.predict(X_test)\n",
        "our_model_prediction = our_model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlwrxZWFkQEA",
        "outputId": "3ad7a717-5f38-43dd-ab05-21176e11de7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "if np.sum(np.abs(keras_prediction - our_model_prediction)) < 0.01:\n",
        "    print('Test PASSED')\n",
        "else:\n",
        "    print('Something went wrong!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxoJ2MSYkQEB"
      },
      "source": [
        "### 2. Вычисление производных по входу для слоёв нейронной сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6AiS7kNkQEB"
      },
      "source": [
        "В данном задании запрещено использовать численные формулы для вычисления производных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaLvS7LokQEB"
      },
      "source": [
        "#### 2.1  Реализуйте метод forward для класса CrossEntropy\n",
        "Напоминание: $$ crossentropy = L(p, y) =  - \\sum\\limits_i y_i log p_i, $$\n",
        "где вектор $(p_1, ..., p_k) $ -  выход классификационного алгоритма, а $(y_1,..., y_k)$ - правильные метки класса в унарной кодировке (one-hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M261iu1ykQEB"
      },
      "outputs": [],
      "source": [
        "class CrossEntropy(object):\n",
        "    def __init__(self, eps=0.00001):\n",
        "        self.name = 'CrossEntropy'\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input_data, labels):\n",
        "        return -np.sum(labels*np.log(input_data))\n",
        "\n",
        "    def calculate_loss(self,input_data, labels):\n",
        "        return self.forward(input_data, labels)\n",
        "\n",
        "    def grad_x(self, input_data, lables):\n",
        "        return -sum(lables/input_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A46sFqkvkQEB"
      },
      "source": [
        "#### 2.2  Реализуйте метод grad_x класса CrossEntropy, который возвращает $\\frac{\\partial L}{\\partial p}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1ZOwEwJkQEB"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uygmYOdxkQEB",
        "outputId": "f228a96d-9f9f-4390-d952-7265c21eb7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_diff_net(net, x, labels):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (net.calculate_loss(x + delta, labels) - net.calculate_loss(x-delta, labels)) / (2*eps)\n",
        "        right_answer.append(diff)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_net(net):\n",
        "    x = np.array([[1, 2, 3], [2, 3, 4]])\n",
        "    labels = np.array([[0.3, 0.2, 0.5], [0.3, 0.2, 0.5]])\n",
        "    num_grad = numerical_diff_net(net, x, labels)\n",
        "    grad = net.grad_x(x, labels)\n",
        "\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "loss = CrossEntropy()\n",
        "test_net(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUXsxyU4kQED"
      },
      "source": [
        "#### 2.3  Реализуйте метод grad_x класса Softmax, который возвращает $\\frac{\\partial Softmax}{\\partial x}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cjN4rYNkQED"
      },
      "source": [
        "Проверить работоспособность кода поможет следующий тест:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53RNOjfUkQED",
        "outputId": "6ba48249-176a-4b5b-e8db-1ec5175efe9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_diff_layer(layer, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(x[0])):\n",
        "        delta = np.zeros(len(x[0]))\n",
        "        delta[i] = eps\n",
        "        diff = (layer.forward(x + delta) - layer.forward(x-delta)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_layer(layer):\n",
        "    x = np.array([[1, 2, 3], [2, -3, 4]])\n",
        "    num_grad = numerical_diff_layer(layer, x)\n",
        "    grad = layer.grad_x(x)\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "layer = Softmax()\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZkXMrHTkQED"
      },
      "source": [
        "#### 2.4  Реализуйте метод grad_x для классов ReLU и DenseLayer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AE-isywCkQED",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ab947ac-ac9b-45b5-d856-ae51b36835f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "layer = ReLU()\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oisB0mTSkQEE",
        "outputId": "062a2855-ac7d-4847-d542-d65ddc5214ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "layer = DenseLayer(3,4)\n",
        "test_layer(layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jj5glHVDkQEE"
      },
      "source": [
        "#### 2.5 (4 балла) Для класса Network реализуйте метод grad_x, который должен реализовывать взятие производной от лосса по входу"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d46WSSVhkQEE"
      },
      "outputs": [],
      "source": [
        "net = Network([DenseLayer(3, 10), ReLU(), DenseLayer(10, 3), Softmax()], loss=CrossEntropy())\n",
        "test_net(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oJsRiUQQkQEE"
      },
      "source": [
        "### 3. Реализация градиентов по параметрам и метода обратного распространения ошибки с обновлением парметров сети"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-eBPdMJkQEE"
      },
      "source": [
        "#### 3.1  Реализуйте функции grad_b и grad_W. При подготовке теста grad_W предполагается, что W является одномерным вектором."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOPvbkcmkQEE",
        "outputId": "e10ed647-107c-45b0-ab95-0633a5922667",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad_b(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(len(b)):\n",
        "        delta = np.zeros(b.shape)\n",
        "        delta[i] = eps\n",
        "        dense1 = DenseLayer(input_size, output_size, W_init=W, b_init=b+delta)\n",
        "        dense2 = DenseLayer(input_size, output_size, W_init=W, b_init=b-delta)\n",
        "        diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "        right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_b():\n",
        "    input_size = 3\n",
        "    output_size = 4\n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((output_size,))\n",
        "    x = np.random.random((2, input_size))\n",
        "\n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_b(x)\n",
        "\n",
        "    num_grad = numerical_grad_b(input_size, output_size, b_init, W_init, x)\n",
        "\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_b()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsZ36sexkQEF",
        "outputId": "24aa14ee-b41f-4497-f936-a1f0abfbe4a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test PASSED\n"
          ]
        }
      ],
      "source": [
        "def numerical_grad_W(input_size, output_size, b, W, x):\n",
        "    eps = 0.00001\n",
        "    right_answer = []\n",
        "    for i in range(W.shape[0]):\n",
        "        for j in range(W.shape[1]):\n",
        "            delta = np.zeros(W.shape)\n",
        "            delta[i, j] = eps\n",
        "            dense1 = DenseLayer(input_size, output_size, W_init=W+delta, b_init=b)\n",
        "            dense2 = DenseLayer(input_size, output_size, W_init=W-delta, b_init=b)\n",
        "            diff = (dense1.forward(x) - dense2.forward(x)) / (2*eps)\n",
        "            right_answer.append(diff.T)\n",
        "    return np.array(right_answer).T\n",
        "\n",
        "def test_grad_W():\n",
        "    input_size = 3\n",
        "    output_size = 4\n",
        "    W_init = np.random.random((input_size, output_size))\n",
        "    b_init = np.random.random((4,))\n",
        "    x = np.random.random((2, input_size))\n",
        "\n",
        "    dense = DenseLayer(input_size, output_size, W_init, b_init)\n",
        "    grad = dense.grad_W(x)\n",
        "\n",
        "    num_grad = numerical_grad_W(input_size, output_size, b_init, W_init, x)\n",
        "\n",
        "    if np.sum(np.abs(num_grad - grad)) < 0.01:\n",
        "        print('Test PASSED')\n",
        "    else:\n",
        "        print('Something went wrong!')\n",
        "        print('Numerical grad is')\n",
        "        print(num_grad)\n",
        "        print('Your gradiend is ')\n",
        "        print(grad)\n",
        "\n",
        "test_grad_W()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLP8zK8ZkQEF"
      },
      "source": [
        "#### 3.2 Полностью реализуйте метод обратного распространения ошибки в функции train_step класса Network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8W81MIjkQEF"
      },
      "source": [
        "Рекомендуем реализовать сначала функцию Network.grad_param(), которая возвращает список длиной в количество слоёв и элементом которого является список градиентов по параметрам.\n",
        "После чего, имея список градиентов, написать функцию обновления параметров для каждого слоя.\n",
        "\n",
        "Совет: рекомендуем написать тест для кода подсчета градиента по параметрам, чтобы быть уверенным в том, что градиент через всю сеть считается правильно\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XB_SB2AkQEF"
      },
      "source": [
        "#### 3.3 Ознакомьтесь с реализацией функции fit класса Network. Запустите обучение модели. Если всё работает правильно, то точность на валидации должна будет возрастать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBpauuP0kQEF"
      },
      "outputs": [],
      "source": [
        "net = Network([DenseLayer(784, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::3], Y_train[::3], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWR1K5rdkQEF"
      },
      "outputs": [],
      "source": [
        "net = Network([DenseLayer(784, 20), ReLU(), DenseLayer(20, 10), Softmax()], loss=CrossEntropy())\n",
        "trainX = X_train.reshape(len(X_train), -1)\n",
        "net.fit(trainX[::6], Y_train[::6], validation_split=0.25,\n",
        "            batch_size=16, nb_epoch=5, learning_rate=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUdbrK1tkQEF"
      },
      "source": [
        "#### 3.5 Продемонстрируйте, что ваша реализация позволяет обучать более глубокие нейронные сети"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1MF_KrpkQEG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hUUCrW9rkQD-",
        "QDQUgDAskQD_",
        "kx5pbvpbkQD_",
        "aaLvS7LokQEB",
        "A46sFqkvkQEB",
        "TUXsxyU4kQED",
        "pZkXMrHTkQED",
        "y-eBPdMJkQEE"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}