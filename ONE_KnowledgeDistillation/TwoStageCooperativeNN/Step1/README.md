# Step1 ‚Äî Two-stage training of a cooperative neural network

Based on: Xu et al., *Knowledge Distillation On the Fly Native Ensemble (ONE)*, NeurIPS 2018.  
Original implementation: [Lan1991Xu/ONE_NeurIPS2018](https://github.com/Lan1991Xu/ONE_NeurIPS2018)

## Background
Deep neural networks achieve strong performance but require resource-intensive training. Knowledge distillation reduces training cost by transferring knowledge from a teacher to a student model. The ONE method eliminates the need for a pre-trained teacher by constructing auxiliary branches and forming an ensemble teacher on the fly.

## Objectives
1. Validate ONE results for RESNET32 on CIFAR100.  
2. Prove feasibility of scaling from 3 to 5 branches without loss of classification quality.  
3. Implement parameter transfer procedure from RESNET32 to extended network.  
4. Extend evaluation metrics (top‚Äë1 and top‚Äë5) to strengthen proof.

## Evolution of Experiments

### üîπ Original Scripts
- **Baseline:** plain SGD, cross‚Äëentropy, top‚Äë1 accuracy only.  
- **ONE:** 3 branches + ensemble, cross‚Äëentropy + KL distillation, top‚Äë1 accuracy per branch.

### üîπ First Modified Pair (‚ÄúAfter LR‚Äù)
- Introduced **GradientRatioScheduler** and **geometric LR updates**.  
- Extended ONE architecture to **5 branches + ensemble**.  
- Loss extended to all branches.  
- Logging expanded with epoch, time, LR.  
- **Proof in practice:** ensemble accuracy ‚â• smaller branch network, confirming p‚ÇÇ ‚â• p‚ÇÅ.

### üîπ Last Modified Pair (‚ÄúWith top‚Äë5 before LR‚Äù)
- Added **top‚Äë5 accuracy tracking** per branch and ensemble.  
- Logging expanded to include both top‚Äë1 and top‚Äë5 metrics.  
- LR schedule adjusted (gamma = 0.333).  
- **Proof in practice:** ensemble top‚Äë5 accuracy consistently matched/exceeded smaller branch networks, reinforcing corollaries.

## Assertion & Proof
- **Assertion:** A 5-branch network (S2) can be initialized from a 3-branch network (S1) such that accuracy p‚ÇÇ ‚â• p‚ÇÅ.  
- **Proof:** By contradiction, zeroing outputs of two branches in S2 reduces it to S1, contradicting p‚ÇÇ < p‚ÇÅ.  
- **Corollaries:**  
  1. S2 classifies no worse than S1.  
  2. S2 can always be extended from S1 without degrading performance.

## Implementation
- `parameter_transfer.py` ‚Üí prepares and transfers parameters from pre-trained model.  
- `cifar_baseline_original.py` / `cifar_one_original.py` ‚Üí original scripts.  
- `cifar_baseline_afterLR.py` / `cifar_one_afterLR.py` ‚Üí first modified pair.  
- `cifar_baseline_top5.py` / `cifar_one_top5.py` ‚Üí last modified pair.  

## Practical Significance
Thanks to the two-stage learning method, as the number of branches increases, training can be faster and accuracy preserved or improved. Extended metrics (top‚Äë5) further validate robustness of cooperative distillation.
