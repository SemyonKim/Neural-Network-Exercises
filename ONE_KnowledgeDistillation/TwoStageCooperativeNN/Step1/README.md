# Step1 â€” Two-stage training of a cooperative neural network
- **Reference Article:** **[Xu et al. *Knowledge Distillation On the Fly Native Ensemble* (ONE) NeurIPS 2018.](https://arxiv.org/pdf/1806.04606)**  
- **Original GitHub Repository:** [Lan1991Xu/ONE_NeurIPS2018](https://github.com/Lan1991Xu/ONE_NeurIPS2018)

---

## ğŸ“– Background
Deep neural networks achieve strong performance but require resource-intensive training. Knowledge distillation reduces training cost by transferring knowledge from a teacher to a student model. The ONE method eliminates the need for a pre-trained teacher by constructing auxiliary branches and forming an ensemble teacher on the fly.

My theoretical contribution:
- **Assertion:** A larger cooperative network (e.g., 5 branches) can be initialized from a smaller one (e.g., 3 branches) such that accuracy $`ğ‘_2 â‰¥ ğ‘_1`$.
- Corollaries:
  1. A 5â€‘branch network classifies no worse than a 3â€‘branch network.
  2. A larger network can always be extended from a smaller one without degrading performance.
 
---

## ğŸ§© Evolution of Scripts
- **Original Pair (Baseline + ONE)**
  - **Baseline** (cifar_baseline_original.py):
    - Standard CIFARâ€‘10/100 training loop.
    - Optimizer: plain SGD with fixed schedule.
    - Loss: crossâ€‘entropy only.
    - Metrics: topâ€‘1 accuracy only.
    - Logging: train/test loss + accuracy.
  - **ONE** (cifar_one_original.py):
    - Implements Xu et al. ONE method.
    - Architecture: 3 branches + ensemble teacher.
    - Loss: crossâ€‘entropy per branch + KL distillation loss from ensemble teacher.
    - Metrics: topâ€‘1 accuracy per branch + ensemble.
    - Logging: branch accuracies + ensemble accuracy.

- **First Modified Pair (â€œAfter LRâ€)**
  - **Baseline** (cifar_baseline_afterLR.py):
    - Added GradientRatioScheduler for parameterâ€‘specific learning rates.
    - Introduced geometric LR updates inside training loop.
    - Added flags: --geo-lr, --deterministic, --save-checkpoint-model.
    - Expanded logging: epoch, time, LR.
    - Still tracked topâ€‘1 accuracy only.
  - **ONE** (cifar_one_afterLR.py):
    - Extended architecture: 3 â†’ 5 branches + ensemble teacher.
    - Loss: crossâ€‘entropy + KL distillation across all 5 branches, ensemble = branch 6.
    - Optimizer replaced with GradientRatioScheduler.
    - Added geometric LR updates (â€œAfter LRâ€).
    - Logging expanded: perâ€‘branch accuracies, ensemble accuracy, epoch, time, LR.

> **Integration with Theory:**  
This implements the scaling proof (S1 â†’ S2, 3 â†’ 5 branches). The custom LR scheduler stabilizes training of the larger cooperative network. Results confirmed ensemble accuracy â‰¥ smaller branch network, validating $`ğ‘_2 â‰¥ ğ‘_1`$.

- **Last Modified Pair (â€œWith Topâ€‘5 Before LRâ€)**
  - **Baseline** (cifar_baseline_top5.py):
    - Added topâ€‘5 accuracy tracking alongside topâ€‘1.
    - Logging now includes both metrics before LR adjustments.
    - Retained standard optimizer (SGD) but gamma changed (0.333).
    - Extended evaluation metrics to validate broader classification performance.
  - **ONE** (cifar_one_top5.py):
    - Architecture: still 5 branches + ensemble.
    - Loss: crossâ€‘entropy + KL distillation across all branches.
    - Metrics: topâ€‘1 and topâ€‘5 accuracy per branch + ensemble.
    - Logging: detailed perâ€‘branch topâ€‘1/topâ€‘5, ensemble topâ€‘1/topâ€‘5.
    - LR adjustment remains standard schedule (not GradientRatioScheduler here).

> **Integration with Theory:**  
By tracking topâ€‘5 accuracies, I validated that scaling branches preserves not only strict topâ€‘1 accuracy but also broader classification quality. This strengthens the corollaries: larger cooperative networks classify no worse than smaller ones.


---

## ğŸ“Š Comparison Table
|Aspect |	Original Scripts (Baseline + ONE) |	First Modified Pair (â€œAfter LRâ€)|
| :--- | :---: | :--- |
| Optimizer	| Plain SGD with fixed schedule |	GradientRatioScheduler with parameterâ€‘specific LR + geometric updates |
| Learning Rate Control | Static schedule (adjust_learning_rate) | Dynamic updates inside training loop (After LR) |
| Architecture | Baseline: single model ONE: 3 branches + ensemble | Extended to 5 branches + ensemble teacher |
| Loss Functions | Baseline: crossâ€‘entropy only ONE: crossâ€‘entropy + KL distillation (3 branches) | Crossâ€‘entropy + KL distillation across 5 branches, ensemble = branch 6 |
| Metrics Tracked | Baseline: topâ€‘1 only ONE: topâ€‘1 per branch + ensemble | Topâ€‘1 per branch + ensemble, epoch, time, LR |
| Logging | Train/test loss + accuracy | Expanded: perâ€‘branch accuracies, ensemble, epoch, time, LR |
| Flags Added | None | --geo-lr, --deterministic, --save-checkpoint-model |
| Connection to Theory | Reproduces ONE baseline results | Implements scaling proof (S1 â†’ S2, 3 â†’ 5 branches) with LR stabilization |

---

## ğŸ§  Practical Proof of Theory
- **Assertion:** A 5â€‘branch network (S2) can be initialized from a 3â€‘branch network (S1) such that accuracy $`ğ‘_2 â‰¥ ğ‘_1`$.
- **Proof in Practice:**
  - Extended scripts to 5 branches.
  - Implemented parameter transfer + cooperative distillation.
  - Logged both topâ€‘1 and topâ€‘5 accuracies across branches.
  - Results confirmed ensemble accuracy â‰¥ smaller branch network, validating theoretical claim.

---

## ğŸ“‚ Folder Structure
- `parameter_transfer.py` â†’ prepares and transfers parameters from pre-trained model.  
- `cifar_baseline_original.py` / `cifar_one_original.py` â†’ original scripts.  
- `cifar_baseline_afterLR.py` / `cifar_one_afterLR.py` â†’ first modified pair.  
- `cifar_baseline_top5.py` / `cifar_one_top5.py` â†’ last modified pair.

---

## Practical Significance
Thanks to the two-stage learning method, as the number of branches increases, training can be faster and accuracy preserved or improved. Extended metrics (topâ€‘5) further validate robustness of cooperative distillation.
